import argparse
import csv
import sys
import logging
import pandas as pd
import json
from sacrebleu import corpus_bleu
#from rouge_score import rouge_scorer, scoring
from typing import  Dict, List
import re
try:
    import nltk

    NLTK_AVAILABLE = True
except (ImportError, ModuleNotFoundError):
    NLTK_AVAILABLE = False
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

logging.basicConfig(
    stream=sys.stdout,
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s ",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
LOGGER = logging.getLogger(__name__)

#BLEU_TOKENIZE={
#    'japanese' : 'ja-mecab',
#    'chinese': 'zh',
#    'hindi': 'indic',
#    'bengali': 'indic',
#    'gujarati': 'indic',
#    'tamil': 'indic',
#    'telugu': 'indic',
#    'marathi': 'indic',
#    'nepali': 'indic',
#    'punjabi': 'indic',
#    'urdu': 'indic',
#    'thai':'th',
#    'vietnamese' : 'vi',
#    'russian' : 'ru',
#    'turkish' : 'tr',
#    'arabic' : 'ar',
#    'korean' : 'ko',
#}
ROUGE_KEYS = ["rouge1", "rouge2", "rougeL", "rougeLsum"]

def extract_rouge_mid_statistics(dct):
    new_dict = {}
    for k1, v1 in dct.items():
        mid = v1.mid
        new_dict[k1] = {stat: round(getattr(mid, stat), 4) for stat in ["precision", "recall", "fmeasure"]}
    return new_dict

def calculate_bleu(output_lns, refs_lns, bleu_lang, **kwargs) -> dict:
    """Uses sacrebleu's corpus_bleu implementation.
    """
    out_list, ref_list = [], []
    for out, ref in zip(output_lns, refs_lns):
        if len(out) != 0  and len(ref) != 0 :
            out_list.append(out)
            ref_list.append(ref)
    assert len(out_list) == len(ref_list)
    #LOGGER.info("*"*100)
    lang_bleu_score = corpus_bleu(out_list, [ref_list], **kwargs)
    LOGGER.info("BLEU Score and Evalution split Size: %s || %d" %(lang_bleu_score, len(ref_list)))
    #LOGGER.info(lang_bleu_score)
    return lang_bleu_score

def add_newline_to_end_of_each_sentence(x: str) -> str:
    """This was added to get rougeLsum scores matching published rougeL scores for BART and PEGASUS."""
    re.sub("<n>", "", x)  # remove pegasus newline char
    assert NLTK_AVAILABLE, "nltk must be installed to separate newlines between sentences. (pip install nltk)"
    return "\n".join(nltk.sent_tokenize(x))

def calculate_rouge(
    pred_lns: List[str],
    tgt_lns: List[str],
    use_stemmer=True,
    rouge_keys=ROUGE_KEYS,
    return_precision_and_recall=False,
    bootstrap_aggregation=True,
    newline_sep=True,
    rouge_lang=None,
) -> Dict:
    """Calculate rouge using rouge_scorer package.

    Args:
        pred_lns: list of summaries generated by model
        tgt_lns: list of groundtruth summaries (e.g. contents of val.target)
        use_stemmer:  Bool indicating whether Porter stemmer should be used to
        strip word suffixes to improve matching.
        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum
        return_precision_and_recall: (False) whether to also return precision and recall.
        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False
            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``
        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL
        on multi sentence summaries (CNN/DM dataset).

    Returns:
         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys

    """
    LOGGER.info("Rouge lang: " + rouge_lang)
    scorer = rouge_scorer.RougeScorer(
        rouge_keys, lang=rouge_lang,
        use_stemmer=use_stemmer
    )
    aggregator = scoring.BootstrapAggregator()
    for pred, tgt in zip(tgt_lns, pred_lns):
        # rougeLsum expects "\n" separated sentences within a summary
        if newline_sep:
            pred = add_newline_to_end_of_each_sentence(pred)
            tgt = add_newline_to_end_of_each_sentence(tgt)
        scores = scorer.score(pred, tgt)
        aggregator.add_scores(scores)

    if bootstrap_aggregation:
        result = aggregator.aggregate()
        if return_precision_and_recall:
            LOGGER.info(extract_rouge_mid_statistics(result))  # here we return dict
        else:
            LOGGER.info({k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()})

    else:
        LOGGER.info(aggregator._scores)  # here we return defaultdict(list)

def calculate_bleu_score(args, preds, refs, prefixes, report_with_prefix_length, bleu_lang):
    args.group1 = [ int(item) for item in args.group1]
    args.group2 = [ int(item) for item in args.group2]
    if report_with_prefix_length:
        pred_13, pred_46, pred_g6 = [],[],[]
        ref_13, ref_46, ref_g6 = [],[],[]
        for i, (pred_item, ref_item, prefix_item) in enumerate(zip(preds, refs, prefixes)):
            prfix_len = len(prefix_item.strip())
            if prfix_len in args.group1:
               pred_13.append(pred_item)
               ref_13.append(ref_item)
            elif prfix_len in args.group2:
               pred_46.append(pred_item)
               ref_46.append(ref_item)
            else:
               pred_g6.append(pred_item)
               ref_g6.append(ref_item)
        assert len(preds) == len(pred_13 + pred_46 + pred_g6), "length of all the prediction list and distributed list should be same"
        assert len(refs) == len(ref_13 + ref_46 + ref_g6), "length of all the referance list and distributed list should be same"

        LOGGER.info("Evalution Scores for Prefix Length Range {} are: ".format(str(args.group1)))
        calculate_bleu(pred_13, ref_13, bleu_lang=bleu_lang)
        LOGGER.info("Evalution Scores for Prefix Length Range {} are: ".format(str(args.group2)))
        calculate_bleu(pred_46, ref_46, bleu_lang=bleu_lang)
        LOGGER.info("Evalution Scores for Prefix Length Range > {} are: ".format(str(args.th_prefix)))
        calculate_bleu(pred_g6, ref_g6, bleu_lang=bleu_lang)
    return calculate_bleu(preds, refs, bleu_lang=bleu_lang) 

def main():
    """ Implementations are partialy taken from XL-Sum Paper"""
    parser = argparse.ArgumentParser(description="APPG metric evaluation script")
    parser.add_argument('--evalFile', action='append', nargs='+', help='path to testing datasets')
    #parser.add_argument("--evalFile", type=str, required=True, help="Path to model genertaed file")
    parser.add_argument("--lang",help="Generated text language", type=str, default='english')
    parser.add_argument('--report_with_prefix_length', help='report the scores for different prefix length', action='store_true')
    parser.add_argument('--group1', nargs="+", default=[1,2,3])
    parser.add_argument('--group2', nargs="+", default=[4,5,6])
    parser.add_argument("--th_prefix",help="all prefixes greater then lenghts", type=int, default=8)

    args = parser.parse_args()
    LOGGER.info(args)

    evalfiles = [(pd.read_csv(fileName, sep='\t', header=None).values.tolist(), fileName) for fileName in args.evalFile[0]]
    
    all_ref, all_pred, all_prefix= [], [], []
    for read_evalfile, fName in evalfiles:
        ref, pred, prefix= [], [], []
        for instance in read_evalfile:
            ref.append(json.loads(instance[0])['reference'])
            pred.append(json.loads(instance[0])['predictions'][0])
            prefix.append(json.loads(instance[0])['prefix'])
        assert len(ref) == len(pred) == len(prefix) , 'reference, prediction and prefix lengths should be same'
        LOGGER.info("Evalution Score for File %s is :", fName)
        bleu_score = calculate_bleu_score(args, pred, ref, prefix, report_with_prefix_length=args.report_with_prefix_length, bleu_lang=args.lang)
        print("*"*100)
        all_ref.extend(ref)
        all_pred.extend(pred)
        all_prefix.extend(prefix)
    assert len(all_ref) == len(all_pred) == len(all_prefix) , 'reference, prediction and prefix lengths should be same'
    LOGGER.info("Overall Evalution Score : %s", fName)
    bleu_score = calculate_bleu_score(args, all_pred, all_ref, all_prefix, report_with_prefix_length=args.report_with_prefix_length, bleu_lang=args.lang)

if __name__ == "__main__":
    main()
